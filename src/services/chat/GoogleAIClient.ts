/**
 * è•¾å§†ç²¾å¿ƒæ„å»ºçš„ Google AI ä¸“ç”¨é€‚é…å™¨
 * âœ¨ è§£å†³ Google AI ä¸ OpenAI æ ¼å¼ä¸å…¼å®¹é—®é¢˜
 *
 * Google AI API æ ¼å¼å®Œå…¨ä¸åŒï¼š
 * - è®¤è¯: x-goog-api-key (ä¸æ˜¯ Authorization: Bearer)
 * - ç«¯ç‚¹: /v1beta/models/{model}:streamGenerateContent
 * - è¯·æ±‚: contents[{role, parts:[{text}]}] (ä¸æ˜¯ messages[{role, content}])
 * - å“åº”: candidates[{content:{parts:[{text}]}}] (ä¸æ˜¯ choices[{message:{content}}])
 * - è§’è‰²: model (ä¸æ˜¯ assistant)
 *
 * @example
 * ```ts
 * import { GoogleAIClient } from '@/services/chat'
 *
 * const client = new GoogleAIClient({
 *   apiKey: 'AIza...',
 *   model: 'gemini-2.5-flash',
 * })
 *
 * await client.chat(messages, callbacks, { temperature: 0.7 })
 * ```
 */

import type {
  ChatCompletionRequest,
  StreamCallbacks,
  ChatError,
} from './types'

// ========================================
// Google AI å®¢æˆ·ç«¯é…ç½®
// ========================================
export interface GoogleAIClientConfig {
  apiKey: string
  baseUrl?: string   // é»˜è®¤: https://generativelanguage.googleapis.com
}

// ========================================
// Google AI API è¯·æ±‚æ ¼å¼
// ========================================
interface GoogleContent {
  role: 'user' | 'model'
  parts: Array<{ text?: string; inline_data?: { mime_type: string; data: string } }>
}

// ğŸ¯ è•¾å§†ï¼šæ€è€ƒé“¾é…ç½®
interface ThinkingConfig {
  includeThoughts?: boolean  // æ˜¯å¦è¿”å›æ€è€ƒæ‘˜è¦
  thinkingBudget?: number     // -1 = åŠ¨æ€, 0 = ç¦ç”¨, >0 = å›ºå®šé¢„ç®—
}

interface GoogleGenerateContentRequest {
  contents: GoogleContent[]
  generationConfig?: {
    temperature?: number
    maxOutputTokens?: number
    thinkingConfig?: ThinkingConfig  // ğŸ¯ è•¾å§†ï¼šæ€è€ƒé“¾é…ç½®
  }
}

// ========================================
// Google AI API å“åº”æ ¼å¼
// ========================================
interface GooglePart {
  text?: string
  thought?: boolean  // ğŸ¯ è•¾å§†ï¼šæ˜¯å¦ä¸ºæ€è€ƒå†…å®¹
}

interface GoogleCandidate {
  content?: {
    parts: Array<GooglePart>
    role: string
  }
  finishReason?: string
  index: number
}

interface GenerateContentResponse {
  candidates: GoogleCandidate[]
  usageMetadata?: {
    promptTokenCount: number
    candidatesTokenCount: number
    totalTokenCount: number
  }
  modelVersion?: string
  responseId?: string
}

// ========================================
// Google AI å®¢æˆ·ç«¯
// ========================================
export class GoogleAIClient {
  private readonly apiKey: string
  private readonly baseUrl: string

  constructor(config: GoogleAIClientConfig) {
    this.apiKey = config.apiKey
    this.baseUrl = config.baseUrl || 'https://generativelanguage.googleapis.com'

    console.log(`ğŸ” è•¾å§†è°ƒè¯•ï¼šGoogleAIClient åˆå§‹åŒ– - baseUrl=${this.baseUrl}`)
  }

  /**
   * æ„å»º API endpoint URL
   * æ ¼å¼: /v1beta/models/{model}:streamGenerateContent?alt=sse
   *
   * ğŸ¯ è•¾å§†å…³é”®ä¿®å¤ï¼šæ·»åŠ  alt=sse å‚æ•°å¯ç”¨çœŸæ­£çš„ SSE æµå¼ï¼
   */
  private buildEndpoint(model: string): string {
    // ç§»é™¤å°¾éƒ¨æ–œæ 
    const baseUrl = this.baseUrl.replace(/\/$/, '')
    return `${baseUrl}/v1beta/models/${model}:streamGenerateContent?alt=sse`
  }

  /**
   * ğŸ¯ è•¾å§†ï¼šåˆ¤æ–­æ¨¡å‹æ˜¯å¦æ”¯æŒæ€è€ƒé“¾
   * Gemini 2.5 Pro/Flash ç³»åˆ—æ”¯æŒ thinking
   */
  private isThinkingModel(model: string): boolean {
    const lowerModel = model.toLowerCase()
    return (
      lowerModel.includes('gemini-2.5-pro') ||
      lowerModel.includes('gemini-2.5-flash') ||
      lowerModel.includes('gemini-2.5-flash-lite') ||
      lowerModel.includes('gemini-3-pro') ||
      lowerModel.includes('gemini-3-flash') ||
      // é€šç”¨åŒ¹é…ï¼šthinking ç³»åˆ—æ¨¡å‹
      lowerModel.includes('thinking')
    )
  }

  /**
   * æ„å»ºè¯·æ±‚å¤´
   * Google AI ä½¿ç”¨ x-goog-api-key header
   */
  private buildHeaders(): HeadersInit {
    return {
      'Content-Type': 'application/json',
      'Accept': 'text/event-stream',  // ğŸ¯ è•¾å§†ï¼šæ˜ç¡®è¦æ±‚ SSE æµå¼å“åº”
      'x-goog-api-key': this.apiKey,
    }
  }

  /**
   * è½¬æ¢ OpenAI æ ¼å¼çš„æ¶ˆæ¯ä¸º Google AI æ ¼å¼
   *
   * OpenAI: { role: 'assistant', content: '...' }
   * Google: { role: 'model', parts: [{ text: '...' }] }
   */
  private convertMessagesToGoogleContents(
    messages: Array<{ role: string; content: string }>
  ): GoogleContent[] {
    return messages.map((msg) => ({
      role: msg.role === 'assistant' ? 'model' : ('user' as const),
      parts: [{ text: msg.content }],
    }))
  }

  /**
   * å‘é€æµå¼èŠå¤©è¯·æ±‚
   *
   * @param messages - OpenAI æ ¼å¼çš„æ¶ˆæ¯å†å²
   * @param callbacks - æµå¼å›è°ƒå‡½æ•°
   * @param options - å¯é€‰å‚æ•°ï¼ˆç³»ç»Ÿæç¤ºè¯ã€æ¸©åº¦ã€æ¨¡å‹ç­‰ï¼‰
   */
  async chat(
    messages: Array<{ role: string; content: string }>,
    callbacks: StreamCallbacks,
    options?: {
      systemPrompt?: string
      temperature?: number
      maxTokens?: number
      model?: string
    }
  ): Promise<void> {
    try {
      const model = options?.model || 'gemini-2.5-flash'
      const url = this.buildEndpoint(model)
      const headers = this.buildHeaders()

      console.log(`ğŸ” è•¾å§†è°ƒè¯•ï¼šGoogle AI å‘é€è¯·æ±‚ - url=${url}, model=${model}`)

      // è½¬æ¢æ¶ˆæ¯æ ¼å¼
      const googleContents = this.convertMessagesToGoogleContents(messages)

      // å¦‚æœæœ‰ç³»ç»Ÿæç¤ºè¯ï¼Œæ’å…¥åˆ°å¼€å¤´ï¼ˆä½œä¸ºç¬¬ä¸€æ¡ user æ¶ˆæ¯ï¼‰
      let finalContents = googleContents
      if (options?.systemPrompt) {
        finalContents = [
          {
            role: 'user' as const,
            parts: [{ text: `ç³»ç»ŸæŒ‡ä»¤ï¼š${options.systemPrompt}` }],
          },
          ...googleContents,
        ]
      }

      const requestBody: GoogleGenerateContentRequest = {
        contents: finalContents,
        generationConfig: {
          temperature: options?.temperature ?? 0.7,
          maxOutputTokens: options?.maxTokens ?? 4096,
          // ğŸ¯ è•¾å§†ï¼šä¸º Pro/Thinking æ¨¡å‹å¯ç”¨æ€è€ƒé“¾
          thinkingConfig: this.isThinkingModel(model) ? {
            includeThoughts: true,   // è¿”å›æ€è€ƒæ‘˜è¦
            thinkingBudget: -1,       // åŠ¨æ€æ€è€ƒé¢„ç®—ï¼ˆæ¨¡å‹è‡ªå·±å†³å®šï¼‰
          } : undefined,
        },
      }

      console.log('ğŸ” è•¾å§†è°ƒè¯•ï¼šGoogle AI è¯·æ±‚ä½“ =', JSON.stringify(requestBody, null, 2))

      const response = await fetch(url, {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody),
      })

      if (!response.ok) {
        await this.handleError(response)
      }

      await this.processStream(response, callbacks)
    } catch (error) {
      callbacks.onError(error as Error)
    }
  }

  /**
   * å¤„ç† SSE æµå¼å“åº”
   *
   * ğŸ¯ è•¾å§†ä¿®å¤ v4ï¼šæ”¯æŒ Gemini æ€è€ƒé“¾ï¼ˆthought å­—æ®µï¼‰
   *
   * SSE æ ¼å¼:
   * data: {"candidates": [{"content": {"parts": [{"text": "...", "thought": true}]}}]}
   */
  private async processStream(
    response: Response,
    callbacks: StreamCallbacks
  ): Promise<void> {
    const reader = response.body?.getReader()
    if (!reader) {
      throw new Error('æ— æ³•è·å–å“åº”æµ')
    }

    const decoder = new TextDecoder('utf-8')
    let buffer = ''

    try {
      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        buffer += decoder.decode(value, { stream: true })

        // æŒ‰è¡Œå¤„ç† SSE æ ¼å¼
        const lines = buffer.split('\n')
        buffer = lines.pop() || ''  // ä¿ç•™æœ€åä¸å®Œæ•´çš„è¡Œ

        for (const line of lines) {
          const trimmed = line.trim()

          // è·³è¿‡ç©ºè¡Œ
          if (!trimmed) continue

          // ç§»é™¤ SSE çš„ "data: " å‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
          const dataStr = trimmed.startsWith('data: ') ? trimmed.slice(6) : trimmed

          try {
            const parsed = JSON.parse(dataStr)

            // å¤„ç†å•ä¸ªå“åº”å¯¹è±¡æˆ–æ•°ç»„
            const items = Array.isArray(parsed) ? parsed : [parsed]

            for (const item of items) {
              const candidate = item.candidates?.[0]
              if (!candidate?.content?.parts) continue

              // ğŸ¯ è•¾å§†å…³é”®ï¼šéå†æ‰€æœ‰ partsï¼Œæ£€æŸ¥ thought å­—æ®µ
              for (const part of candidate.content.parts) {
                if (part.text) {
                  if (part.thought === true) {
                    // è¿™æ˜¯æ€è€ƒé“¾å†…å®¹
                    if (callbacks.onReasoningChunk) {
                      callbacks.onReasoningChunk(part.text)
                    }
                  } else {
                    // è¿™æ˜¯æ­£å¸¸å›ç­”å†…å®¹
                    callbacks.onChunk(part.text)
                  }
                }
              }

              // æ£€æŸ¥æ˜¯å¦å®Œæˆ
              const finishReason = candidate.finishReason
              if (finishReason && finishReason !== 'IN_PROGRESS') {
                callbacks.onComplete()
                return
              }
            }
          } catch (parseError) {
            console.debug('è§£æ SSE è¡Œå¤±è´¥ï¼Œè·³è¿‡:', parseError, dataStr.slice(0, 100))
          }
        }
      }

      callbacks.onComplete()
    } catch (error) {
      console.error('Google AI æµå¤„ç†é”™è¯¯:', error)
      callbacks.onError(error as Error)
    } finally {
      reader.releaseLock()
    }
  }

  /**
   * ç»Ÿä¸€é”™è¯¯å¤„ç†
   */
  private async handleError(response: Response): Promise<never> {
    let errorMessage = 'è¯·æ±‚å¤±è´¥'
    let errorType = 'unknown_error'

    try {
      const errorData: ChatError = await response.json()
      errorMessage = errorData.error?.message || errorMessage
      errorType = errorData.error?.type || errorType
    } catch {
      errorMessage = response.statusText || errorMessage
    }

    // æ·»åŠ ä¾›åº”å•†ä¿¡æ¯åˆ°é”™è¯¯æ¶ˆæ¯
    errorMessage = `[Google AI] ${errorMessage}`
    throw new Error(errorMessage)
  }
}

// é‡æ–°å¯¼å‡ºç±»å‹
export type { ChatCompletionRequest, StreamCallbacks, ChatError } from './types'
